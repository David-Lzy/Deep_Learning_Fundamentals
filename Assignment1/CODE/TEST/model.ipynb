{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       " 0     0.639530  0.865481      -0.031969       0.670206 -0.181423  0.166511   \n",
       " 1    -0.844335 -1.204281      -0.527975      -0.012293 -0.181423 -0.851645   \n",
       " 2     1.233077  2.015348      -0.693310      -0.012293 -0.181423 -1.331632   \n",
       " 3    -0.844335 -1.072868      -0.527975      -0.694792 -0.540290 -0.633469   \n",
       " 4    -1.141108  0.504094      -2.677331       0.670206  0.316360  1.548294   \n",
       " \n",
       "    DiabetesPedigreeFunction       Age  \n",
       " 0                  0.468187  1.425067  \n",
       " 1                 -0.364823 -0.190548  \n",
       " 2                  0.604004 -0.105515  \n",
       " 3                 -0.920163 -1.040871  \n",
       " 4                  5.481337 -0.020483  ,\n",
       " 0    1\n",
       " 1    0\n",
       " 2    1\n",
       " 3    0\n",
       " 4    1\n",
       " Name: Outcome, dtype: int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reload the standardized dataset\n",
    "data_path = '../Database/standardized_diabetes_data.csv'\n",
    "standardized_data = pd.read_csv(data_path)\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = standardized_data.drop(columns='Outcome')\n",
    "y = standardized_data['Outcome']\n",
    "\n",
    "# Display the first few rows of features and target variable\n",
    "X.head(), y.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Partition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/envs/torch-gpu/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda/anaconda3/envs/torch-gpu/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((614, 8), (154, 8), (614,), (154,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Display the shape of the training and testing sets to confirm the split\n",
    "(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the Perceptron model\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        # Define the single linear layer\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear layer\n",
    "        # and then through the sigmoid activation function\n",
    "        out = torch.sigmoid(self.fc1(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build method how to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model, \n",
    "        data, \n",
    "        learning_rate=0.001,\n",
    "        batch_size=32,\n",
    "        model_p = dict()\n",
    "        ):\n",
    "\n",
    "        self.training_loss = []\n",
    "        self.training_accuracy = []\n",
    "        self.testing_accuracy = []\n",
    "\n",
    "        X_train, X_test, y_train, y_test = data\n",
    "        # Prepare the data for PyTorch\n",
    "        X_train_tensor = Variable(torch.Tensor(X_train.values))\n",
    "        y_train_tensor = Variable(torch.Tensor(y_train.values))\n",
    "        X_test_tensor = Variable(torch.Tensor(X_test.values))\n",
    "        y_test_tensor = Variable(torch.Tensor(y_test.values))\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset=train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True)\n",
    "        self.test_data = X_test_tensor, y_test_tensor\n",
    "        \n",
    "        self.model = model(X_train.shape[1],**model_p)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        total_loss = 0\n",
    "        correct_train_preds = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for inputs, targets in self.train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = self.model(inputs)\n",
    "            # Compute the loss\n",
    "            loss = self.criterion(outputs, targets.view(-1, 1))\n",
    "            total_loss += loss.item() * len(targets)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # Compute the number of correct predictions for training accuracy\n",
    "            correct_train_preds += ((outputs > 0.5).type(torch.FloatTensor).view(-1) == targets).sum().item()\n",
    "            total_train_samples += len(targets)\n",
    "\n",
    "        return total_loss, total_train_samples, correct_train_preds\n",
    "\n",
    "    def evaluate(self, total_loss, total_train_samples, correct_train_preds, epoch, num_epochs):\n",
    "        # Compute training loss and accuracy\n",
    "        avg_train_loss = total_loss / total_train_samples\n",
    "        train_acc = correct_train_preds / total_train_samples\n",
    "        self.training_loss.append(avg_train_loss)\n",
    "        self.training_accuracy.append(train_acc)\n",
    "\n",
    "        # Compute testing accuracy\n",
    "        with torch.no_grad():\n",
    "            test_outputs = self.model(self.test_data[0])\n",
    "            test_preds = (test_outputs > 0.5).type(torch.FloatTensor)\n",
    "            test_acc = accuracy_score(self.test_data[1], test_preds)\n",
    "            self.testing_accuracy.append(test_acc)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_train_loss:.4f}, Training Accuracy: {train_acc * 100:.2f}%, Testing Accuracy: {test_acc * 100:.2f}%')\n",
    "\n",
    "    def run(self, num_epochs, evaluation_interval):\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss, total_train_samples, correct_train_preds = self.train()\n",
    "            # Evaluate the model every evaluation_interval epochs\n",
    "            if (epoch + 1) % evaluation_interval == 0:\n",
    "                self.evaluate(total_loss, total_train_samples, correct_train_preds, epoch, num_epochs)\n",
    "                self.model.train()  # Switch back to training mode\n",
    "\n",
    "# Usage:\n",
    "# trainer = Trainer(model, train_loader, (X_test_tensor, y_test_tensor))\n",
    "# trainer.run(num_epochs=1000, evaluation_interval=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 0.6237, Training Accuracy: 66.78%, Testing Accuracy: 58.44%\n",
      "Epoch [20/200], Loss: 0.5508, Training Accuracy: 74.10%, Testing Accuracy: 68.18%\n",
      "Epoch [30/200], Loss: 0.5139, Training Accuracy: 75.73%, Testing Accuracy: 70.13%\n",
      "Epoch [40/200], Loss: 0.4936, Training Accuracy: 76.71%, Testing Accuracy: 70.78%\n",
      "Epoch [50/200], Loss: 0.4811, Training Accuracy: 77.20%, Testing Accuracy: 70.78%\n",
      "Epoch [60/200], Loss: 0.4728, Training Accuracy: 77.36%, Testing Accuracy: 70.13%\n",
      "Epoch [70/200], Loss: 0.4673, Training Accuracy: 77.85%, Testing Accuracy: 70.78%\n",
      "Epoch [80/200], Loss: 0.4634, Training Accuracy: 77.85%, Testing Accuracy: 70.78%\n",
      "Epoch [90/200], Loss: 0.4606, Training Accuracy: 78.18%, Testing Accuracy: 70.13%\n",
      "Epoch [100/200], Loss: 0.4587, Training Accuracy: 78.50%, Testing Accuracy: 70.78%\n",
      "Epoch [110/200], Loss: 0.4571, Training Accuracy: 78.66%, Testing Accuracy: 70.78%\n",
      "Epoch [120/200], Loss: 0.4561, Training Accuracy: 78.99%, Testing Accuracy: 70.13%\n",
      "Epoch [130/200], Loss: 0.4553, Training Accuracy: 79.15%, Testing Accuracy: 70.13%\n",
      "Epoch [140/200], Loss: 0.4548, Training Accuracy: 79.15%, Testing Accuracy: 69.48%\n",
      "Epoch [150/200], Loss: 0.4544, Training Accuracy: 79.15%, Testing Accuracy: 69.48%\n",
      "Epoch [160/200], Loss: 0.4541, Training Accuracy: 79.32%, Testing Accuracy: 68.83%\n",
      "Epoch [170/200], Loss: 0.4539, Training Accuracy: 79.64%, Testing Accuracy: 69.48%\n",
      "Epoch [180/200], Loss: 0.4536, Training Accuracy: 79.64%, Testing Accuracy: 69.48%\n",
      "Epoch [190/200], Loss: 0.4535, Training Accuracy: 79.64%, Testing Accuracy: 68.83%\n",
      "Epoch [200/200], Loss: 0.4533, Training Accuracy: 79.80%, Testing Accuracy: 69.48%\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    Perceptron,\n",
    "    (X_train, X_test, y_train, y_test),\n",
    "    )\n",
    "\n",
    "trainer.run(num_epochs=200, evaluation_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. make a MLP for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim = 16):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define the first linear layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)  # New Linear Layer\n",
    "        # Define the last linear layer\n",
    "        self.fc_last = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the first linear layer\n",
    "        # and then through the ReLU activation function\n",
    "        x1 = torch.relu(self.fc1(x))\n",
    "        x2 = torch.relu(self.fc2(x1))\n",
    "        x3 = torch.relu(self.fc3(x2))  # Passing through new Linear Layer\n",
    "        # Pass the output through the last linear layer\n",
    "        # and then through the sigmoid activation function\n",
    "        out = torch.sigmoid(self.fc_last(x3))\n",
    "        return out\n",
    "\n",
    "# Assuming input_dim is the number of features in your input data\n",
    "input_dim = 8  # Replace with the actual number of features\n",
    "hidden_dim = 16  # You can choose a different size for the hidden layer\n",
    "\n",
    "# Create an instance of the MLP class\n",
    "mlp = MLP(input_dim, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    MLP, \n",
    "    (X_train, X_test, y_train, y_test),\n",
    "    model_p={\n",
    "        'hidden_dim':32\n",
    "    }\n",
    "    )\n",
    "trainer.run(num_epochs=200, evaluation_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Test for more fixable MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP_fix(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(MLP_fix, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # 添加第一个线性层（从输入维度到隐藏维度）\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        # 添加剩下的隐藏层\n",
    "        for i in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        # 添加最后一个线性层（从隐藏维度到输出维度）\n",
    "        self.fc_last = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Pass the input through the linear layer\n",
    "            # and then through the ReLU activation function\n",
    "            x = torch.relu(layer(x))\n",
    "        # Pass the output through the last linear layer\n",
    "        # and then through the sigmoid activation function\n",
    "        out = torch.sigmoid(self.fc_last(x))\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 0.4385, Training Accuracy: 78.99%, Testing Accuracy: 70.13%\n",
      "Epoch [20/200], Loss: 0.4173, Training Accuracy: 79.80%, Testing Accuracy: 70.13%\n",
      "Epoch [30/200], Loss: 0.4038, Training Accuracy: 80.78%, Testing Accuracy: 71.43%\n",
      "Epoch [40/200], Loss: 0.3929, Training Accuracy: 81.92%, Testing Accuracy: 72.08%\n",
      "Epoch [50/200], Loss: 0.3836, Training Accuracy: 81.60%, Testing Accuracy: 73.38%\n",
      "Epoch [60/200], Loss: 0.3750, Training Accuracy: 82.25%, Testing Accuracy: 74.03%\n",
      "Epoch [70/200], Loss: 0.3672, Training Accuracy: 82.90%, Testing Accuracy: 73.38%\n",
      "Epoch [80/200], Loss: 0.3604, Training Accuracy: 83.88%, Testing Accuracy: 72.73%\n",
      "Epoch [90/200], Loss: 0.3546, Training Accuracy: 84.04%, Testing Accuracy: 75.32%\n",
      "Epoch [100/200], Loss: 0.3479, Training Accuracy: 84.20%, Testing Accuracy: 74.68%\n",
      "Epoch [110/200], Loss: 0.3418, Training Accuracy: 84.04%, Testing Accuracy: 73.38%\n",
      "Epoch [120/200], Loss: 0.3364, Training Accuracy: 84.85%, Testing Accuracy: 73.38%\n",
      "Epoch [130/200], Loss: 0.3305, Training Accuracy: 84.36%, Testing Accuracy: 73.38%\n",
      "Epoch [140/200], Loss: 0.3247, Training Accuracy: 85.50%, Testing Accuracy: 74.03%\n",
      "Epoch [150/200], Loss: 0.3175, Training Accuracy: 86.16%, Testing Accuracy: 74.03%\n",
      "Epoch [160/200], Loss: 0.3125, Training Accuracy: 85.99%, Testing Accuracy: 74.68%\n",
      "Epoch [170/200], Loss: 0.3111, Training Accuracy: 86.81%, Testing Accuracy: 76.62%\n",
      "Epoch [180/200], Loss: 0.3002, Training Accuracy: 86.97%, Testing Accuracy: 74.68%\n",
      "Epoch [190/200], Loss: 0.2981, Training Accuracy: 87.30%, Testing Accuracy: 74.68%\n",
      "Epoch [200/200], Loss: 0.2897, Training Accuracy: 87.30%, Testing Accuracy: 74.03%\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    MLP_fix, \n",
    "    (X_train, X_test, y_train, y_test),\n",
    "    model_p={\n",
    "        'hidden_dim':128,\n",
    "        'num_layers':1\n",
    "    }\n",
    "    )\n",
    "trainer.run(num_epochs=200, evaluation_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
